{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier that's embarrassingly simple to train, it is literally just counting, yet works surprisingly well in practice, especially for text.\n",
    "\n",
    "The \"naive\" part refers to a simplifying assumption about feature independence. This assumption is almost always wrong, but the algorithm works \"well enough\" most of the times. Understanding *why*, requires us to dig into probability theory.\n",
    "\n",
    "**What we'll cover:**\n",
    "- Generative vs discriminative models\n",
    "- Bayes' theorem (properly derived)\n",
    "- The conditional independence assumption\n",
    "- Training by counting\n",
    "- Laplace smoothing\n",
    "- Different variants (Gaussian, Multinomial, Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs Discriminative Models\n",
    "\n",
    "There are two fundamentally different approaches to classification:\n",
    "\n",
    "**Discriminative models** (like logistic regression) learn $P(Y|X)$ directly:\n",
    "- \"Given these features, what's the probability of each class?\"\n",
    "- Learn the decision boundary between classes\n",
    "- Don't model how data is generated\n",
    "\n",
    "**Generative models** (like Naive Bayes) learn $P(X|Y)$ and $P(Y)$:\n",
    "- \"What do features look like for each class?\"\n",
    "- Model the distribution of data within each class\n",
    "- Can generate synthetic examples (in principle)\n",
    "\n",
    "Naive Bayes is generative: it learns what spam emails look like, what ham emails look like, then uses Bayes' theorem to flip things around for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem: The Foundation\n",
    "\n",
    "For classification, we want $P(Y|X)$, the probability of class $Y$ given features $X$.\n",
    "\n",
    "**Bayes' theorem** lets us express this in terms of things we can estimate:\n",
    "\n",
    "$$P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}$$\n",
    "\n",
    "Let's understand each term:\n",
    "\n",
    "| Term | Name | Meaning |\n",
    "|------|------|--------|\n",
    "| $P(Y|X)$ | **Posterior** | What we want: probability of class given features |\n",
    "| $P(X|Y)$ | **Likelihood** | How likely these features are for this class |\n",
    "| $P(Y)$ | **Prior** | How common this class is overall |\n",
    "| $P(X)$ | **Evidence** | How common these features are (any class) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Bayes' Theorem to Classification\n",
    "\n",
    "For classification, we compare $P(Y=c|X)$ across all classes $c$ and pick the highest.\n",
    "\n",
    "**Key insight:** The denominator $P(X)$ is the same for all classes!\n",
    "\n",
    "$$P(Y=\\text{spam}|X) = \\frac{P(X|Y=\\text{spam}) \\cdot P(Y=\\text{spam})}{P(X)}$$\n",
    "\n",
    "$$P(Y=\\text{ham}|X) = \\frac{P(X|Y=\\text{ham}) \\cdot P(Y=\\text{ham})}{P(X)}$$\n",
    "\n",
    "Since we're only comparing (not computing exact probabilities), we can ignore $P(X)$:\n",
    "\n",
    "$$\\hat{Y} = \\arg\\max_c \\; P(X|Y=c) \\cdot P(Y=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: High-Dimensional Features\n",
    "\n",
    "If $X = (X_1, X_2, ..., X_d)$ is a vector of $d$ features, we need:\n",
    "\n",
    "$$P(X_1, X_2, ..., X_d | Y=c)$$\n",
    "\n",
    "This is the joint distribution of all features for class $c$. \n",
    "\n",
    "**Problem:** Estimating joint distributions requires exponentially many parameters!\n",
    "\n",
    "Example: 1000 binary features → $2^{1000}$ possible combinations: tanti auguri.\n",
    "\n",
    "We need a simplifying assumption..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Assumption: Conditional Independence\n",
    "\n",
    "**Assumption:** Features are conditionally independent given the class.\n",
    "\n",
    "$$P(X_1, X_2, ..., X_d | Y) = P(X_1|Y) \\cdot P(X_2|Y) \\cdot ... \\cdot P(X_d|Y) = \\prod_{i=1}^{d} P(X_i|Y)$$\n",
    "\n",
    "**What this means:** Once you know the class, knowing one feature tells you nothing about another.\n",
    "\n",
    "**Example (spam detection):**\n",
    "- If an email is spam, the probability it contains \"free\" doesn't depend on whether it contains \"winner\"\n",
    "- This is obviously false! Spam emails with \"free\" are more likely to also have \"winner\"\n",
    "- But the assumption makes the math tractable\n",
    "\n",
    "### The Complete Model\n",
    "\n",
    "Combining everything:\n",
    "\n",
    "$$\\hat{Y} = \\arg\\max_c \\; P(Y=c) \\prod_{i=1}^{d} P(X_i|Y=c)$$\n",
    "\n",
    "This is the **Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Just Counting\n",
    "\n",
    "Both components are estimated by counting:\n",
    "\n",
    "**Prior probability:**\n",
    "$$P(Y=c) = \\frac{\\text{number of examples in class } c}{\\text{total examples}}$$\n",
    "\n",
    "**Likelihood (for categorical features):**\n",
    "$$P(X_i = v | Y = c) = \\frac{\\text{count of examples where } X_i=v \\text{ and } Y=c}{\\text{count of examples where } Y=c}$$\n",
    "\n",
    "That's it. No optimization, no gradient descent. Just counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Email Classification\n",
      "============================================================\n",
      "\n",
      "Total emails: 10\n",
      "  Spam: 4 (40%)\n",
      "  Ham:  6 (60%)\n",
      "\n",
      "Prior probabilities:\n",
      "  P(spam) = 4/10 = 0.40\n",
      "  P(ham)  = 6/10 = 0.60\n"
     ]
    }
   ],
   "source": [
    "# Complete worked example: Email spam classification\n",
    "# Simplified dataset with word presence/absence\n",
    "\n",
    "print(\"Training Data: Email Classification\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Training data: [has_free, has_winner, has_meeting, has_report]\n",
    "emails = [\n",
    "    # Spam emails\n",
    "    {'free': 1, 'winner': 1, 'meeting': 0, 'report': 0, 'label': 'spam'},\n",
    "    {'free': 1, 'winner': 0, 'meeting': 0, 'report': 0, 'label': 'spam'},\n",
    "    {'free': 1, 'winner': 1, 'meeting': 0, 'report': 0, 'label': 'spam'},\n",
    "    {'free': 0, 'winner': 1, 'meeting': 0, 'report': 0, 'label': 'spam'},\n",
    "    # Ham emails  \n",
    "    {'free': 0, 'winner': 0, 'meeting': 1, 'report': 1, 'label': 'ham'},\n",
    "    {'free': 0, 'winner': 0, 'meeting': 1, 'report': 0, 'label': 'ham'},\n",
    "    {'free': 1, 'winner': 0, 'meeting': 1, 'report': 1, 'label': 'ham'},\n",
    "    {'free': 0, 'winner': 0, 'meeting': 0, 'report': 1, 'label': 'ham'},\n",
    "    {'free': 0, 'winner': 0, 'meeting': 1, 'report': 1, 'label': 'ham'},\n",
    "    {'free': 0, 'winner': 0, 'meeting': 1, 'report': 0, 'label': 'ham'},\n",
    "]\n",
    "\n",
    "# Count\n",
    "n_spam = sum(1 for e in emails if e['label'] == 'spam')\n",
    "n_ham = sum(1 for e in emails if e['label'] == 'ham')\n",
    "n_total = len(emails)\n",
    "\n",
    "print(f\"Total emails: {n_total}\")\n",
    "print(f\"  Spam: {n_spam} ({n_spam/n_total:.0%})\")\n",
    "print(f\"  Ham:  {n_ham} ({n_ham/n_total:.0%})\")\n",
    "\n",
    "# Priors\n",
    "p_spam = n_spam / n_total\n",
    "p_ham = n_ham / n_total\n",
    "\n",
    "print(f\"\\nPrior probabilities:\")\n",
    "print(f\"  P(spam) = {n_spam}/{n_total} = {p_spam:.2f}\")\n",
    "print(f\"  P(ham)  = {n_ham}/{n_total} = {p_ham:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods P(word=1 | class):\n",
      "============================================================\n",
      "\n",
      "'free':\n",
      "  P('free'=1 | spam) = 3/4 = 0.75\n",
      "  P('free'=1 | ham)  = 1/6 = 0.17\n",
      "\n",
      "'winner':\n",
      "  P('winner'=1 | spam) = 3/4 = 0.75\n",
      "  P('winner'=1 | ham)  = 0/6 = 0.00\n",
      "\n",
      "'meeting':\n",
      "  P('meeting'=1 | spam) = 0/4 = 0.00\n",
      "  P('meeting'=1 | ham)  = 5/6 = 0.83\n",
      "\n",
      "'report':\n",
      "  P('report'=1 | spam) = 0/4 = 0.00\n",
      "  P('report'=1 | ham)  = 4/6 = 0.67\n"
     ]
    }
   ],
   "source": [
    "# Calculate likelihoods for each word\n",
    "words = ['free', 'winner', 'meeting', 'report']\n",
    "\n",
    "print(\"Likelihoods P(word=1 | class):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "likelihoods = {'spam': {}, 'ham': {}}\n",
    "\n",
    "for word in words:\n",
    "    # Count word occurrences in each class\n",
    "    count_in_spam = sum(1 for e in emails if e['label'] == 'spam' and e[word] == 1)\n",
    "    count_in_ham = sum(1 for e in emails if e['label'] == 'ham' and e[word] == 1)\n",
    "    \n",
    "    p_word_given_spam = count_in_spam / n_spam\n",
    "    p_word_given_ham = count_in_ham / n_ham\n",
    "    \n",
    "    likelihoods['spam'][word] = p_word_given_spam\n",
    "    likelihoods['ham'][word] = p_word_given_ham\n",
    "    \n",
    "    print(f\"\\n'{word}':\")\n",
    "    print(f\"  P('{word}'=1 | spam) = {count_in_spam}/{n_spam} = {p_word_given_spam:.2f}\")\n",
    "    print(f\"  P('{word}'=1 | ham)  = {count_in_ham}/{n_ham} = {p_word_given_ham:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying new email:\n",
      "  Words present: ['free', 'meeting']\n",
      "  Words absent:  ['winner', 'report']\n",
      "\n",
      "============================================================\n",
      "\n",
      "P(spam | email) is proportional to:\n",
      "  P(spam) = 0.40 × P('free'=1|spam) = 0.75 × P('winner'=0|spam) = 0.25 × P('meeting'=1|spam) = 0.00 × P('report'=0|spam) = 1.00\n",
      "  = 0.000000\n",
      "\n",
      "P(ham | email) is proportional to:\n",
      "  P(ham) = 0.60 × P('free'=1|ham) = 0.17 × P('winner'=0|ham) = 1.00 × P('meeting'=1|ham) = 0.83 × P('report'=0|ham) = 0.33\n",
      "  = 0.027778\n",
      "\n",
      "============================================================\n",
      "Normalized probabilities:\n",
      "  P(spam | email) = 0.000000 / 0.027778 = 0.0%\n",
      "  P(ham | email)  = 0.027778 / 0.027778 = 100.0%\n",
      "\n",
      "Prediction: HAM\n"
     ]
    }
   ],
   "source": [
    "# Classify a new email\n",
    "new_email = {'free': 1, 'winner': 0, 'meeting': 1, 'report': 0}\n",
    "\n",
    "print(\"Classifying new email:\")\n",
    "print(f\"  Words present: {[w for w, v in new_email.items() if v == 1]}\")\n",
    "print(f\"  Words absent:  {[w for w, v in new_email.items() if v == 0]}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "def compute_score(label, email, priors, likelihoods):\n",
    "    \"\"\"Compute P(label) * product of P(features | label)\"\"\"\n",
    "    score = priors[label]\n",
    "    terms = [f\"P({label}) = {priors[label]:.2f}\"]\n",
    "    \n",
    "    for word, present in email.items():\n",
    "        if present == 1:\n",
    "            p = likelihoods[label][word]\n",
    "            terms.append(f\"P('{word}'=1|{label}) = {p:.2f}\")\n",
    "        else:\n",
    "            p = 1 - likelihoods[label][word]\n",
    "            terms.append(f\"P('{word}'=0|{label}) = {p:.2f}\")\n",
    "        score *= p\n",
    "    \n",
    "    return score, terms\n",
    "\n",
    "priors = {'spam': p_spam, 'ham': p_ham}\n",
    "\n",
    "# Compute for spam\n",
    "score_spam, terms_spam = compute_score('spam', new_email, priors, likelihoods)\n",
    "print(\"\\nP(spam | email) is proportional to:\")\n",
    "print(f\"  {' × '.join(terms_spam)}\")\n",
    "print(f\"  = {score_spam:.6f}\")\n",
    "\n",
    "# Compute for ham\n",
    "score_ham, terms_ham = compute_score('ham', new_email, priors, likelihoods)\n",
    "print(\"\\nP(ham | email) is proportional to:\")\n",
    "print(f\"  {' × '.join(terms_ham)}\")\n",
    "print(f\"  = {score_ham:.6f}\")\n",
    "\n",
    "# Normalize to get actual probabilities\n",
    "total = score_spam + score_ham\n",
    "prob_spam = score_spam / total\n",
    "prob_ham = score_ham / total\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Normalized probabilities:\")\n",
    "print(f\"  P(spam | email) = {score_spam:.6f} / {total:.6f} = {prob_spam:.1%}\")\n",
    "print(f\"  P(ham | email)  = {score_ham:.6f} / {total:.6f} = {prob_ham:.1%}\")\n",
    "print(f\"\\nPrediction: {'SPAM' if prob_spam > prob_ham else 'HAM'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Zero-Frequency Problem\n",
    "\n",
    "What if a word never appears in spam during training, but appears in a test email?\n",
    "\n",
    "$$P(\\text{word}|\\text{spam}) = \\frac{0}{n_{\\text{spam}}} = 0$$\n",
    "\n",
    "Since we multiply probabilities:\n",
    "\n",
    "$$P(\\text{spam}|\\text{email}) \\propto P(\\text{spam}) \\times ... \\times 0 \\times ... = 0$$\n",
    "\n",
    "One unseen word zeros out the entire probability! This is catastrophic.\n",
    "\n",
    "### Solution: Laplace Smoothing\n",
    "\n",
    "Add a small count $\\alpha$ (typically 1) to every feature count:\n",
    "\n",
    "$$P(X_i = v | Y = c) = \\frac{\\text{count}(X_i = v, Y = c) + \\alpha}{\\text{count}(Y = c) + \\alpha \\cdot |V|}$$\n",
    "\n",
    "Where $|V|$ is the number of possible values for feature $X_i$.\n",
    "\n",
    "**Effect:** No probability is ever exactly 0 or 1.\n",
    "\n",
    "**Note:**  $\\alpha \\cdot |V|$ at the denominator allows normalization, effectively mantaining the score a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Probabilities: Avoiding Underflow\n",
    "\n",
    "Another practical issue: multiplying many small probabilities causes numerical underflow.\n",
    "\n",
    "$$P(Y|X) \\propto P(Y) \\prod_{i=1}^{d} P(X_i|Y)$$\n",
    "\n",
    "With 1000 features and $P(X_i|Y) \\approx 0.01$, we'd compute $0.01^{1000} \\approx 10^{-2000}$.\n",
    "\n",
    "**Solution:** Work in log space.\n",
    "\n",
    "$$\\log P(Y|X) \\propto \\log P(Y) + \\sum_{i=1}^{d} \\log P(X_i|Y)$$\n",
    "\n",
    "Products become sums, which are numerically stable. We compare log-probabilities instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Does The Naive Assumption Fail?\n",
    "\n",
    "The independence assumption is almost always violated. When does it matter?\n",
    "\n",
    "**Usually okay:**\n",
    "- Text classification (words have complex dependencies, but NB still works)\n",
    "- When you just need a ranking (relative probabilities), not calibrated probabilities\n",
    "- High-dimensional sparse data\n",
    "\n",
    "**Problematic:**\n",
    "- Duplicate or highly correlated features (double-counting evidence)\n",
    "- When you need accurate probability estimates\n",
    "- Low-dimensional data where other models work better anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Core idea** | Use Bayes' theorem: $P(Y|X) \\propto P(X|Y) \\cdot P(Y)$ |\n",
    "| **Naive assumption** | Features independent given class: $P(X|Y) = \\prod_i P(X_i|Y)$ |\n",
    "| **Training** | Count frequencies to estimate $P(Y)$ and $P(X_i|Y)$ |\n",
    "| **Prediction** | Pick class with highest $P(Y) \\prod_i P(X_i|Y)$ |\n",
    "| **Laplace smoothing** | Add $\\alpha$ to counts to avoid zero probabilities |\n",
    "| **Log probabilities** | Use sums of logs to avoid numerical underflow |\n",
    "\n",
    "**Variants:**\n",
    "- Gaussian: continuous features, model as normal distribution\n",
    "- Multinomial: count features (word counts in text)\n",
    "- Bernoulli: binary features (word presence/absence)\n",
    "\n",
    "**When to use:**\n",
    "- Text classification (spam, sentiment, topic)\n",
    "- High-dimensional sparse data\n",
    "- Need a fast baseline\n",
    "- Limited training data\n",
    "\n",
    "**Limitations:**\n",
    "- Independence assumption often wrong\n",
    "- Probability estimates can be poorly calibrated\n",
    "- Correlated features cause problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
